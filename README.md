# text-mining

#Reflections
#supportive community, functionality, production ready 
1. Project overview: 
I used Wikipedia as my data source and chose spacy.io as my library for more complex analysis of the text. I chose "cancel culture" as the wikipedia page to analyze; because is an controversial topic, I wanted to learn if the author put their own subjective into writing a wikipedia page for cancel culture. 

2. Implementation: 
I broke the project down to three parts. 1. Figuring out what data source I want to deal with. 2. What are the basic functions (We learned in class) I want to use to create some sort of value in my text-analysis. 3. What are the complex functions I want implement?
1) I actually started with the e-book, but half way through iterating the e-book, I found out that Python actually read the whole file as an string, instead of broken down line by line. Therefore, it makes reading the file into a list and a dictionary extremely hard. Therefore, I switched to my second choice, which was a wikipedia page. Wikipedia page was extremly friendly, the content only included the article plus some references, AND it was could be read line by line. 
2) I chose to deal with word frequency in the articles. I wanted to see what kind of words appears in the most in my subject, I felt like it could tell the most about the content of the article(I was wrong but I was explain in "results" section). I did not consider other plans for this part, because I felt like what I did showed the topics we've learned for the last two weeks. 
3)I chose spacy.io as my library to analyze the sentiment of the article. I first wanted to use NLTK, but after talking to Pujit, he recommended that I look into some other libraries that are built on top of NLTK, which are easier to use. Pujit used Spacy.io, which he recommended me to use too for a few reasons: It is already in production; Spacy.io has a good supporting community (I found blogs and good answers on some of the spacy's documentations); and a somewhat easy to understand documentations. 

3. Results
Tracking the frequency of the appearance of the words did not really help analyzing the content of the wikipedia page. At the beginning, I was only going to show the top 10 most appeared words in that page, but it all ended up being words that don't contribute any value to undertand what the article is about. Including (the, of, ot, and, is, by, as, or, for) as few of the top ones. Therefore, I changed it to top thirty, which showed a bit about what the article is about, for example, "public" showed up 11 times, "people" showed up 10 times. From this analysis, we could see that this article is definitely a lot of the public's actions and views, which is what cancel culture is about. 

The second part of the analysis includes the polarity and the subjectivity of the article, which is my main goal for choosing a wikipedia page of a controversial topic. According to: https://www.analyticsvidhya.com/blog/2018/02/natural-language-processing-for-beginners-using-textblob/, "Polarity is float which lies in the range of [-1,1] where 1 means positive statement and -1 means a negative statement. Subjective sentences generally refer to personal opinion, emotion or judgment whereas objective refers to factual information. Subjectivity is also a float which lies in the range of [0,1]." So the result is that the polarity of this article is 0.0925, and a subjectivity of 0.41. These data points shows that this wikipedia page included some of subjective but generally have kept the polarity of the subject to neutral. This is shows that wikipedia could be a somewhat of an valuable source to understand a certain sujects, because it does not contain too much bias or polarization. After the sentiments were printed, I also printed how spacy library evaluated the polarity and subjectivity throughout the article. Which I still don't have a lot of understanding of. 


4. Reflection
There was almost no parts that went smoothly. It took a lot of finishing part of the code for it to not work, taking 1 hour or so to fix just to end up switching (data source and reading the datasource into a dictionary). In addition to using an outside library (spacy), and reading complex documentation isn't something I am extremly familar with. However, I do consider my project to my apprtioriately scoped, I think my code could apply to many wikipedia pages to mass evaluate the average polarity and subjectivity across wikipedia pages to get a scientific conclusion on if wikiedia is a unbiased source to learn. I have learned a ton from this project, such as testing data sources; using print statements to see where the error happened (which i deleted for cleaniness sake); reading documentations for libraries; installing outside libraries and utilizing pre-written functions to simplify my coding process. Additionally, using the spacy library made me feel like I can utilize the proucts of people who are much smarter than me to empower my analysis process, and do more advanced things even without be an expert at software engineering. 
